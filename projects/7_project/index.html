<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Matrix Multiplication Optimization | Madhusudan Agarwal </title> <meta name="author" content="Madhusudan Agarwal"> <meta name="description" content="Optimizing GEMM performance across GPU and CPU architectures"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hero1601.github.io/projects/7_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Madhusudan</span> Agarwal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Matrix Multiplication Optimization</h1> <p class="post-description">Optimizing GEMM performance across GPU and CPU architectures</p> </header> <article> <p>This project focused on building and tuning high-performance matrix multiplication (GEMM) kernels for both NVIDIA GPUs (using CUDA) and ARM-based CPUs (using SVE intrinsics). The goal was to significantly improve over naive baselines, approach vendor-optimized libraries (cuBLAS, OpenBLAS), and understand the hardware/software co-design required for top GEMM speed.</p> <hr> <h2 id="techniques-used">Techniques Used</h2> <h4 id="cuda-kernel--nvidia-gpu">CUDA Kernel ‚Äî NVIDIA GPU</h4> <ul> <li> <strong>Shared Memory Tiling</strong>: Reduces global memory calls by packing submatrices of A and B into shared memory tiles, minimizing redundant loads.</li> <li> <strong>Instruction &amp; Thread-Level Parallelism</strong>: Each thread computes multiple C entries (up to 25) via register blocking and unrolled loops for peak arithmetic throughput.</li> <li> <strong>Memory Coalescing</strong>: All memory loads/stores are coordinated to maximize memory bandwidth and minimize transaction overhead.</li> <li> <strong>Dynamic Tile/Block Sizing</strong>: Systematic parameter sweeps across tile/block sizes (including rectangles) to balance shared memory, register use, and occupancy.</li> <li> <strong>Careful Edge-Case Handling</strong>: Padding and conditional logic on tile boundaries to avoid memory faults and keep all threads productive.</li> </ul> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cuda-480.webp 480w,/assets/img/cuda-800.webp 800w,/assets/img/cuda-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cuda.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" title="CUDA" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p class="text-center small">CUDA Optimized</p> </div> <h4 id="arm-sve-optimization--arm-cpu">ARM SVE Optimization ‚Äî ARM CPU</h4> <ul> <li> <strong>Packing Routines</strong>: Transform row-major input blocks into cache- and SIMD-friendly layouts for fast vector loads/stores.</li> <li> <strong>4√ó4 SVE Microkernel</strong>: Vectorized inner kernel using SVE intrinsics (<code class="language-plaintext highlighter-rouge">svld1_f64</code>, <code class="language-plaintext highlighter-rouge">svmla_f64_m</code>, <code class="language-plaintext highlighter-rouge">svst1_f64</code>). Predicate masks (<code class="language-plaintext highlighter-rouge">svbool_t</code>) prevent out-of-bounds errors.</li> <li> <strong>Loop Unrolling &amp; Minimal Branching</strong>: Maximizes register and instruction utilization while minimizing small-iteration overhead and branch costs.</li> <li> <strong>Cache-Aware Blocking</strong>: Tuning of <code class="language-plaintext highlighter-rouge">Mc</code>, <code class="language-plaintext highlighter-rouge">Kc</code>, <code class="language-plaintext highlighter-rouge">Nc</code>, <code class="language-plaintext highlighter-rouge">Mr</code>, and <code class="language-plaintext highlighter-rouge">Nr</code> for high L1/L2 cache residency; best results with <code class="language-plaintext highlighter-rouge">Mc=128</code>, <code class="language-plaintext highlighter-rouge">Kc=32</code>, <code class="language-plaintext highlighter-rouge">Nc=128</code>, <code class="language-plaintext highlighter-rouge">Mr=4</code>, <code class="language-plaintext highlighter-rouge">Nr=4</code>.</li> <li> <strong>Automated Parameter Search</strong>: A Python script batch-tested configurations to empirically determine best block sizes for each hardware platform.</li> </ul> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cpu-480.webp 480w,/assets/img/cpu-800.webp 800w,/assets/img/cpu-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cpu.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" title="CPU" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p class="text-center small">CPU Optimized</p> </div> <hr> <h2 id="-pseudocode">üìù Pseudocode</h2> <h4 id="cuda-shared-memory--register-tiling">CUDA: Shared Memory + Register Tiling</h4> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cuda-code-480.webp 480w,/assets/img/cuda-code-800.webp 800w,/assets/img/cuda-code-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cuda-code.png" class="img-fluid rounded z-depth-1 w-60" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h4 id="arm-sve">ARM SVE</h4> <h5 id="packing"><strong>Packing</strong></h5> <div class="col-sm mt-3 mt-md-0" style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/packing-480.webp 480w,/assets/img/packing-800.webp 800w,/assets/img/packing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/packing.png" class="img-fluid rounded z-depth-1 w-60" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This function packs matrix A by converting it from row-major to column-major order. The outer loop runs over rows (m ‚â§ Mr), and the inner loop over columns (k ‚â§ Kc). It stores columns of XA as rows in packA, effectively transposing the block for better memory access during computation.</p> <h5 id="sve-kernel"><strong>SVE Kernel</strong></h5> <p>First create a predicate to handle elements from 0 to n</p> <p>svbool_t pg = svwhilelt_b64_u64(0, n);</p> <p>Then check whether we are within the range of m (rows). If we are, then load that row of matrix C from memory. We do this four times because our matrix is 4x4.</p> <p>c0 = (m &gt; 0) ? svld1_f64(pg, c + 0 * ldc) : svdup_f64(0.0);</p> <p>c1 = (m &gt; 1) ? svld1_f64(pg, c + 1 * ldc) : svdup_f64(0.0);</p> <p>c2 = (m &gt; 2) ? svld1_f64(pg, c + 2 * ldc) : svdup_f64(0.0);</p> <p>c3 = (m &gt; 3) ? svld1_f64(pg, c + 3 * ldc) : svdup_f64(0.0);</p> <p>Next, iterate over k (shared dimension) and load a sve vector from matrix b. This vector is protected by the predicate and multiples the current k with n to get the value in the vector.</p> <p>svfloat64_t b0 = svld1_f64(pg, b + l * n);</p> <p>Then, access the first value from matrix A, ensuring it‚Äôs in bounds, represented by (a + l *m) followed by a sve multiply-accumulate operation between the scalar from A (represented as a vector), and the vector B. Then update c0 with this value. To move on to the next scalar in A, we do *(currentVal + 1), etc.</p> <p>double *currentVal = (a + l * m);</p> <p>if(m&gt;0) {</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aval = *currentVal;
svfloat64_t ax = svdup_f64(aval);
c0 = svmla_f64_m(pg, c0, b0, ax);
</code></pre></div></div> <p>}</p> <p>Finally, conditionally store the result in matrix C only for the valid rows</p> <p>if (m &gt; 0) svst1_f64(pg, c + 0 * ldc, c0);</p> <hr> <h2 id="-engineering-highlights">üß™ Engineering Highlights</h2> <h4 id="on-gpu-cuda">On GPU (CUDA)</h4> <ul> <li>Optimal block size of <strong>16√ó16</strong> threads and tile size of <strong>80√ó80</strong> elements maximize occupancy.</li> <li>Rectangular tiles improved occupancy but required careful boundary handling.</li> <li> <code class="language-plaintext highlighter-rouge">#pragma unroll</code> enabled better FMA instruction pipeline utilization.</li> <li>Shared memory tiling and memory coalescing significantly reduce global memory latency.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/thread-blocks.png" alt="CUDA Block Size vs GFLOPS" style="max-width: 100%; height: auto;"> <p><em>Figure: CUDA Block Size vs GFLOPS</em></p> </div> <hr> <h4 id="on-cpu-arm-sve">On CPU (ARM SVE)</h4> <ul> <li>Best cache blocking parameters: <code class="language-plaintext highlighter-rouge">Mc=128</code>, <code class="language-plaintext highlighter-rouge">Kc=32</code>, <code class="language-plaintext highlighter-rouge">Nc=128</code>.</li> <li>Vectorized microkernel doubled performance (from ~6 GFLOPS to ~19 GFLOPS).</li> <li>Predicate-based edge-case handling ensured correctness without branch penalties.</li> <li>Automated parameter tuning was essential to find the sweet spots.</li> </ul> <div class="text-center"> <img src="/assets/img/sve_comparison.png" alt="SVE Packing vs Optimized Microkernel" class="img-fluid" style="max-width: 100%; height: auto;"> <p class="small text-muted">Figure: CPU SVE performance comparison</p> </div> <hr> <h2 id="-performance-summary">üìä Performance Summary</h2> <table> <thead> <tr> <th>Architecture &amp; Kernel</th> <th>Matrix Size (N)</th> <th>Peak GFLOPS</th> <th>Configuration</th> </tr> </thead> <tbody> <tr> <td>ARM SVE (C7g.medium)</td> <td>256</td> <td>18.6</td> <td>Mc=128, Kc=32, Nc=128</td> </tr> <tr> <td>¬†</td> <td>512</td> <td>19.345</td> <td>Mc=128, Kc=32, Nc=128</td> </tr> <tr> <td>¬†</td> <td>1024</td> <td>19.14</td> <td>Mc=128, Kc=32, Nc=128</td> </tr> <tr> <td>¬†</td> <td>2048</td> <td>18.6</td> <td>Mc=128, Kc=32, Nc=128</td> </tr> <tr> <td><strong>‚Äî</strong></td> <td><strong>‚Äî</strong></td> <td><strong>‚Äî</strong></td> <td><strong>‚Äî</strong></td> </tr> <tr> <td>NVIDIA T4 (CUDA)</td> <td>256</td> <td>1121.2</td> <td>block=16√ó16, tile=80√ó80</td> </tr> <tr> <td>¬†</td> <td>512</td> <td>2253.0</td> <td>block=16√ó16, tile=80√ó80</td> </tr> <tr> <td>¬†</td> <td>1024</td> <td>3245.7</td> <td>block=16√ó16, tile=80√ó80</td> </tr> <tr> <td>¬†</td> <td>2048</td> <td>3535.6</td> <td>block=16√ó16, tile=80√ó80</td> </tr> </tbody> </table> <hr> <h2 id="-key-insights">üöÄ Key Insights</h2> <ul> <li>Optimal performance relies on balancing compute/blocking with memory hierarchy, not just making all parameters big.</li> <li>Predicates &amp; packing make SIMD kernels robust to edge cases on CPUs.</li> <li>Systematic tuning (rather than hand-guessing) is crucial for non-trivial kernels across platforms.</li> <li>Speedup: GPU achieved <strong>&gt;100√ó</strong> over naive, and SVE CPU achieved <strong>&gt;3√ó</strong> over packing-only.</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Madhusudan Agarwal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>